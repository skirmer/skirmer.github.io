



---
date: 2023-11-15
featured_image: "https://cdn-images-1.medium.com/max/1024/0*4xW77dr71-j8MXKQ"
tags: ["authenticity","generative-ai-tools","artificial-intelligence","explainable-ai","editors-pick"]
title: "Detecting Generative AI Content"
disable_share: false
---
      
#### 
 On deepfakes, authenticity, and the Presidentâ€™s Executive Order onÂ AI



 One of the many interesting ethical issues that comes with the advances of generative AI is
 **detection of the product of models** 
 . Itâ€™s a practical issue as well, for those of us who consume media. Is this thing I am reading or looking at the product of a personâ€™s thoughtful work, or just words or images probabilistically generated to appeal to me? Does it matter? If so, what do weÂ do?



### 
 The Meaning of Indistinguishable Content



 When we talk about content being difficult or impossible to detect as AI generated, weâ€™re getting into something akin to a Turing Test. Say that I give you a paragraph of text or an image file. If I ask you, â€œwas this produced by a human being or a machine learning model?â€, and you canâ€™t accurately tell, then weâ€™ve reached the point where we need to think about theseÂ issues.




 In many areas weâ€™re close to this, especially with GPT-4, but even with less sophisticated models, depending on what kind of prompt we use and the volume of context. If we have a long document from a GPT model, itâ€™s likely going to be easier to detect that itâ€™s not human generated, because every new word is a chance for the model to do something that a regular person wouldnâ€™t do. Same for a video or high-res imageâ€Šâ€”â€Šthe more chances for pixellation or the uncanny valley to show up, the more opportunity for us to spot theÂ fake.




 It also seems clear to me that as we get more familiar with model generated content, weâ€™re going to get better at identifying tell tale signs of AI involvement in the content. Just like I described several weeks ago in my explanation of how Generative Adversarial Networks work, we are in something of a GAN relationship with generative AI ourselves. The models are working on creating the most human-like content possible, and we are increasing our ability to tell that itâ€™s not human. Itâ€™s like a race where each side is working to outwit theÂ other.




> 
>  As we get more familiar with model generated content, weâ€™re going to get better at identifying tell tale signs of AI involvement in theÂ content.
> 


### 
 Detection Approaches



 However, thereâ€™s probably a limit to how good we can get at this detection, and models will win out over regular human eyes and ears (if they havenâ€™t already). We just donâ€™t have the sensory capabilities and the pattern-recognition sophistication that a giant model can have. Fortunately, we can use models as tools on our side of the race as well, training models to scrutinize content and determine that it isnâ€™t human generated, so thatâ€™s one tool in ourÂ quiver.




 At some point, though, there may really not be any reliably detectable signs of machine learning origins for some content, especially in small quantities. Philosophically speaking, given unlimited model advancement, there is probably a point at which there IS no real difference between the two kinds of content. In addition, most of us wonâ€™t be using a model to test all the media we consume to make sure itâ€™s human generated and authentic. In response, some organizations, such as the
 [Content Authenticity Initiative](https://contentauthenticity.org/) 
 , are driving efforts to get content authentication metadata adopted broadly, which could be a help. These kinds of efforts, however, require goodwill and work on the part of people making the models available.




> 
>  At some point, though, there may really not be any reliably detectable signs of machine learning origins for someÂ content.
> 



 You might ask, well, what about people who are intentionally bad actors trying to create harm with deepfakes or misinformation using AI generated content? Theyâ€™re not going to volunteer to identify their content, are they? Itâ€™s a fair question. However, at least at this moment, the models that are sophisticated enough to fool people at scale are mostly under the control of large companies (OpenAI, etc). This wonâ€™t continue to be the case, but right this minute it would at least make a solid dent in the issue of provenance if the people who serve the most sophisticated LLMs to the public took some action onÂ this.




 This isnâ€™t a very optimistic story, so far. Generative AI is careening rapidly towards a place where those very powerful models are small enough that bad actors can run their own, and where those models easily create content that is literally indistinguishable from organic human content, even by otherÂ models.



### 
 Reasons for Detecting



 Iâ€™ve gotten a little ahead of myself, though. Why is it so important to everyone that we figure out whether content came from a model in the first place? If you canâ€™t tell, does itÂ matter?




 One major reason is that the purveyor of the content may have malicious intentions, such as misinformation or deepfakes. Creating images, audio, and video are the most common scenarios hereâ€Šâ€”â€Šmaking it appear like someone said or did something they didnâ€™t do. If youâ€™ve been following the
 [US Presidentâ€™s Executive Order on AI](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/) 
 , you may have heard that President Biden in fact got interested in this because he heard of the potential for people to use his likeness and voice fraudulently for misinformation. This is a very serious issue, because at this time, we tend to trust what we see with our own eyes in images or videos, and that can have a significant impact on peopleâ€™s lives andÂ safety.




> 
>  At this time, we tend to trust what we see with our own eyes in images or videos, and that can have a significant impact on peopleâ€™s lives andÂ safety.
> 



 A related issue is when models are used to imitate the work of specific humans not for necessarily malicious reasons, but just because that work is enjoyable and popular, and potentially profitable. Itâ€™s straightforwardly unethical, but in most cases probably not meant to actively hurt either the audience or the person being mimicked. (It does hurt people, of courseâ€Šâ€”â€Štaking away the earning potential and livelihoods of artists and writers, which producers of the content should take responsibility for.) This can also cause reputational damage when deepfakes are used to lie about peopleâ€™s actions. (Just ask
 [Joe Rogan, whoâ€™s been fighting ads](https://mashable.com/article/joe-rogan-tiktok-deepfake-ad) 
 using his likeness in deepfakes.)




 A third angle that I began thinking about after Casey Newton discussed it in his
 [October 5 issue of Platformer](https://open.substack.com/pub/platformer/p/how-authoritarian-governments-are?r=7xzvu&utm_medium=ios&utm_campaign=post) 
 is the risk that public figures can invert the issue and claim that real, authentic evidence of their bad behavior is artificially generated. What do we do when we canâ€™t reliably uncover misdeeds using evidence, because â€œitâ€™s a deepfakeâ€ is an unfalsifiable response? Weâ€™re not quite there yet, but I could see this becoming a real issue in the not too distantÂ future.




> 
>  What do we do when we canâ€™t reliably uncover misdeeds using evidence, because â€œitâ€™s a deepfakeâ€ is an unfalsifiable response?
> 



 Less urgently, I also think thereâ€™s something to wanting my consumption of media to represent engaging with another person, even though it is a mainly one way movement of ideas. I think of reading or consuming art as engagement with the thoughts of another person, and engaging with words strung together by a model doesnâ€™t quite have the same feel. So personally, Iâ€™d like to know if content Iâ€™m consuming is human produced.



### 
 The Executive Order



 Given all these very real risks, we are facing some serious challenges. There seems to be a tradeoff between detectable provenance (meaning, safety for the general public and all the issues I described above) and model sophistication, and as an industry, data science is driving forward on the side of model sophistication. Whoâ€™s going to balance the scalesÂ out?




 The presidentâ€™s executive order represents some significant progress on this topic. (It talks about a lot of other important issues as well, which I may discuss another time.) I spent the last week and a half thinking about this executive order and reading other peopleâ€™s perspectives on it from around the industry. While there are some who argue that itâ€™s going to stifle advancement (and that it will reify the big players in generative AI, at the expense of smaller competitors), I think Iâ€™ve landed on the side of being optimistic about the executive order.




 Creating competitive generative AI models is incredibly expensive, and resource intensive, and this naturally limits how many competitors can get into the space in the first place. Protecting hypothetical new players in the space at the expense of broader social safety doesnâ€™t make sense to me. I also donâ€™t think the executive order is at all onerous for those organizations who have the resources to be in this room in the firstÂ place.




 The order itself is also not that prescriptive. It calls for a number of things to be created, but leaves open wide latitude for how that should happen, and hopefully well informed people will be involved in those processes. ðŸ¤ž (Data scientists in the field ought to make a point to monitor what happens and be vocal if itâ€™s going off the rails.) In particular, it asks for the Commerce Department to create â€œstandards and best practices for detecting AI-generated content and authenticating official contentâ€. There are also some important components around safety and security where models are concerned.




**Do I have tremendous trust that our government will do a great job regulating AI while not damaging innovation?** 
**No, not really.** 
 But, I feel confident that industry left to its own devices will not pay as much attention to the issues of provenance and content detection as is neededâ€Šâ€”â€Šthey havenâ€™t demonstrated that this is a priority soÂ far.




> 
>  I feel confident that industry left to its own devices will not pay as much attention to the issues of provenance and content detection as isÂ needed.
> 



 At the same time, Iâ€™m not sure that detecting generative AI produced content will in fact be physically possible in all or even most contexts, especially as we make advances. The executive order doesnâ€™t say anything about preventing models from being developed if their content crosses into undetectable territory, but that risk does worry me. That really would be stifling innovation, and we need to think very carefully about what the tradeoffs are or could be. Although, the horse may be out of that particular barn alreadyâ€Šâ€”â€Šso many open source generative AI models are out there in the world, continuously getting better at what theyÂ do.



### 
 Conclusion



 This topic is hard, and the right action to take is not necessarily very clear. The more sophisticated and complex the output of a model is, the better our chances of detecting that output is not human-generated, but weâ€™re in a technological race that will make that detection harder and harder. Policy engagement on the topic may give us some guard rails, but we canâ€™t know yet whether it will really help, or if it will turn out to be a ham-fisted disaster.




 This is one of those times when I donâ€™t have a way to tie up the discussion in a neat bow. The potential and actual risks of indistinguishable generative AI output are serious, and should be treated as such. However, we are in a scientific/mathematical place where we canâ€™t create a quick or easy solution, and we need to give weight to the benefits that more advanced generative AI couldÂ produce.




 Regardless, data scientists should take time to read the executive order or at least the factsheet, and be clear about what the statement does and does not say. As regular readers will know, I think weâ€™ve got to be responsible for spreading accurate and accessible information about these issues to the laypersons in our lives, and this is a good opportunity, since the topic is in the news. Make sure you contribute positively to the data science literacy aroundÂ you.




 See more of my work at
 [www.stephaniekirmer.com](https://www.stephaniekirmer.com) 
 .



### 
 References



 Casey Newton -
 [October 5 issue of Platformer](https://open.substack.com/pub/platformer/p/how-authoritarian-governments-are?r=7xzvu&utm_medium=ios&utm_campaign=post) 




[The Fact Sheet on the Executive Order onÂ AI](https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/) 




[The full document of the Executive Order onÂ AI](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/) 




[Reporting on deepfake fraud in ads, featuring JoeÂ Rogan](https://mashable.com/article/joe-rogan-tiktok-deepfake-ad) 




[Content Authenticity Initiative](https://contentauthenticity.org/) 






---



[Detecting Generative AI Content](https://towardsdatascience.com/detecting-generative-ai-content-286200498f93) 
 was originally published in
 [Towards Data Science](https://towardsdatascience.com) 
 on Medium, where people are continuing the conversation by highlighting and responding to this story.



