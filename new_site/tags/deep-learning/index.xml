<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep-learning on Stephanie Kirmer</title>
    <link>/tags/deep-learning/</link>
    <description>Recent content in deep-learning on Stephanie Kirmer</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What Does It Mean When Machine Learning Makes a Mistake?</title>
      <link>/writing/whatdoesitmeanwhenmachinelearningmakesamistake/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/writing/whatdoesitmeanwhenmachinelearningmakesamistake/</guid>
      <description>Do our definitions of “mistake” make sense when it comes to ML/AI? If not, why not?
A comment on my recent post about the public perception of machine learning got me thinking about the meaning of error in machine learning. The reader asked if I thought machine learning models would always “make mistakes”. As I described in that post, people have a strong tendency to anthropomorphize machine learning models. When we interact with an LLM chatbot, we apply techniques to those engagements that we have learned by communicating with other people—persuasion, phrasing, argument, etc.</description>
    </item>
  </channel>
</rss>
